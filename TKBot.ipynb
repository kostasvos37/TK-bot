{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TKBot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEhHih09ZasQ"
      },
      "source": [
        "\n",
        "#TKBot\n",
        "\n",
        "Machine learning music generation with LSTM's. Based heavily on [this article](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5) by Towards Data Science. The main improvement is the addition of some rhythmic features to the music generation.\n",
        "\n",
        "Trained on guitar tracks by Ling Tosite Sigure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Yk61qipYIK"
      },
      "source": [
        "The midi tracks are located in a Drive folder, so Google drive must be mounted first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6djalowZSNh",
        "outputId": "162eefc2-a290-4b96-ad5f-e4cc0e431312"
      },
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D50lHmCna-6b"
      },
      "source": [
        "from music21 import converter, instrument, note, chord, stream\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint, Callback\n",
        "import glob\n",
        "from fractions import Fraction \n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uFiYoY1mxiG"
      },
      "source": [
        "# Creating Network Inputs/Outputs from Dataset\n",
        "\n",
        "We begin by creating the network input. We use the library [music21](https://web.mit.edu/music21/doc/index.html) to parse the midi files and isolate the notes and chords.\n",
        "\n",
        "To improve the accuracy and performance of the network, we only consider the following quarter note values, and round all other values to the nearest available:\n",
        "\n",
        "* 0 : x (dead note)\n",
        "\n",
        "* 1: 16th\n",
        "\n",
        "* 2 : 8th\n",
        "\n",
        "* 3 : quatrter\n",
        "\n",
        "* 4 : half\n",
        "\n",
        "The inputs can be either chords or notes. In the case of notes, we represent them in the form \"note dur\", for example \"B- 2\" is a b flat with a duration of an 8th. In the case of chords we use their [normal order](https://web.mit.edu/music21/doc/moduleReference/moduleChord.html#music21.chord.Chord.normalOrder), where each note is seperated by a '.'. For example, the normal order of a C chord is [0, 3, 7], so we represent it as \"0.3.7 dur\". Finally, the dead notes are represented by a \"R\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0J1xseRxP3m"
      },
      "source": [
        "path = \"/content/drive/MyDrive/lts_dataset/*.*\"\n",
        "notes = []\n",
        "for filepath in glob.iglob(path):\n",
        "    print(filepath)\n",
        "    midi = converter.parse(filepath)\n",
        "    notes_to_parse = midi.flat.notes\n",
        "    for element in notes_to_parse:\n",
        "        res = \"\"\n",
        "        \n",
        "        if isinstance(element, note.Note):\n",
        "            res += str(element.pitch)\n",
        "        elif isinstance(element, chord.Chord):\n",
        "            res += '.'.join(str(n) for n in element.normalOrder)\n",
        "        if (element.duration.quarterLength == Fraction(2,3)):\n",
        "            res += \" 2\"\n",
        "        if (element.duration.quarterLength == 0.0):\n",
        "            res = \"R\"\n",
        "        elif (element.duration.quarterLength < 0.5):\n",
        "            res += \" 1\"\n",
        "        elif (element.duration.quarterLength < 1):\n",
        "            res += \" 2\"\n",
        "        elif (element.duration.quarterLength == 1):\n",
        "            res += \" 3\"\n",
        "        else:\n",
        "            res += \" 4\"\n",
        "        notes.append(res)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h81iTYrsqQq"
      },
      "source": [
        "The inputs are sequences of 100 notes, followed by a single output note. We use a hot one representation for the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXzB1nSdqDpP"
      },
      "source": [
        "# getting the unique note-duration values, and creating a map\n",
        "pitches = sorted(set(x for x in notes))\n",
        "n_classes = len(set(notes))\n",
        "\n",
        "noteMap = dict((pitches[i], i) for i in range(len(pitches)))\n",
        "\n",
        "network_input = []\n",
        "network_output = []\n",
        "\n",
        "\n",
        "# generating sequences of 100 notes and the corresponding output\n",
        "sequence_length = 100\n",
        "\n",
        "for i in range(0, len(notes) - sequence_length, 1):\n",
        "    sequence_in = notes[i:i + sequence_length]\n",
        "    network_input.append([noteMap[x] for x in sequence_in])\n",
        "\n",
        "    sequence_out = notes[i + sequence_length]\n",
        "    network_output.append(noteMap[sequence_out])\n",
        "\n",
        "# reshaping for input and normalizing\n",
        "n_in = len(network_input)\n",
        "network_input = numpy.reshape(network_input, (n_in, sequence_length, 1))\n",
        "network_input = network_input / float(n_classes)\n",
        "\n",
        "network_output = np_utils.to_categorical(network_output)\n",
        "print(\"Input size : \"+str(len(network_input)))\n",
        "print(\"Unique classes : \" + str(n_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agIxH-_B3KRb"
      },
      "source": [
        "#Model\n",
        "\n",
        "The model we use contains three LSTM layers, followed by two fully connected layers, with the nodes shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3QzaGmP3E1G"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(\n",
        "    256,\n",
        "    input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "    return_sequences=True\n",
        "))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(256))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(n_classes))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCffAZdFzvJl"
      },
      "source": [
        "weights = \"/content/drive/MyDrive/weights/57-0.2068.h5\"\n",
        "if(len(weights)>0): model.load_weights(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy8i0G1RupW0"
      },
      "source": [
        "Callback function to save the state of the model after every 30 epochs. We train the model for 200 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av7L4mXFuFQG"
      },
      "source": [
        "class CustomSaver(Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % 30 == 0:  \n",
        "            self.model.model(\"/content/drive/My Drive/weights/model_{}_v4.hd5\".format(epoch))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11YwL2DP7vDc"
      },
      "source": [
        "#saving\n",
        "saver = CustomSaver()\n",
        "\n",
        "model.fit(network_input, network_output, callbacks = [saver], epochs=200, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Muw22AD2u4H8"
      },
      "source": [
        "#Generating Music\n",
        "\n",
        "Finally, we get to make some music! We generate 500 note sequences based on the network's output. Starting at a random network input, we generate a note based on the model prediction. To get the next sequence, we cut the first note and append the generated note to the end of the current input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux2reJv8BrZ4"
      },
      "source": [
        "def generate_sequence(net_input):\n",
        "    start = numpy.random.randint(0, len(network_input)-1)\n",
        "    print(start)\n",
        "    revNoteMap = dict((i, pitches[i]) for i in range(len(pitches)))\n",
        "    pattern = net_input[start]\n",
        "    prediction_output = []\n",
        "    # generate 500 notes\n",
        "    for note_index in range(500):\n",
        "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction_input = prediction_input / float(n_classes)\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = revNoteMap[index]\n",
        "        prediction_output.append(result)\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "    return prediction_output"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2AllRpskOuZ"
      },
      "source": [
        "# Recreating input\n",
        "\n",
        "network_input = []\n",
        "network_output = []\n",
        "\n",
        "sequence_length = 50\n",
        "for i in range(0, len(notes) - sequence_length, 1):\n",
        "    sequence_in = notes[i:i + sequence_length]\n",
        "    network_input.append([noteMap[x] for x in sequence_in])\n",
        "\n",
        "    sequence_out = notes[i + sequence_length]\n",
        "    network_output.append(noteMap[sequence_out])\n",
        "\n",
        "prediction_output = generate_sequence(network_input)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WaHFIAevk0l"
      },
      "source": [
        "Finally, we create a midi file based on the generated output. For each prediction, we seperate the note value and the duration. We then create the appropriate object (Note/Chord), with the correct duration. Finally, we increase the offset, so that the next note is in its right place. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44KTaOpBEPFL"
      },
      "source": [
        "offset = 0\n",
        "output_notes = []\n",
        "duration_dict = {'0' : 0.0, '1':0.25, '2':0.5, '3':1, '4':2}\n",
        "# create note and chord objects based on the values generated by the model\n",
        "for pattern in prediction_output:\n",
        "\n",
        "    if (pattern == \"R\"):\n",
        "        new_note = note.Note('A', quarterLength=0.0)\n",
        "        new_note.offset = offset\n",
        "        new_note.storedInstrument = instrument.Piano()\n",
        "        output_notes.append(new_note)\n",
        "        offset += 0.25\n",
        "        continue\n",
        "\n",
        "    sp = pattern.split(\" \")\n",
        "    val = sp[0]\n",
        "    dur = duration_dict[sp[1]]\n",
        "    # pattern is a chord\n",
        "    if ('.' in val) or val.isdigit():\n",
        "\n",
        "        notes_in_chord = val.split('.')\n",
        "        notes = []\n",
        "        for current_note in notes_in_chord:\n",
        "            new_note = note.Note(int(current_note))\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            notes.append(new_note)\n",
        "        new_chord = chord.Chord(notes, quarterLength=dur)\n",
        "        new_chord.offset = offset\n",
        "        output_notes.append(new_chord)\n",
        "    else:\n",
        "        new_note = note.Note(val, quarterLength=dur)\n",
        "        new_note.offset = offset\n",
        "        new_note.storedInstrument = instrument.Piano()\n",
        "        output_notes.append(new_note)\n",
        "    # increase offset each iteration so that notes do not stack\n",
        "    offset += dur\n",
        "\n",
        "midi_stream = stream.Stream(output_notes)\n",
        "midi_stream.write('midi', fp='output.mid')"
      ],
      "execution_count": 54,
      "outputs": []
    }
  ]
}